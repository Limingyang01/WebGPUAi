# 布丁离线 AI 助手 - 项目文档

## 一、项目概述

### 1.1 项目简介

布丁离线 AI 助手是一个基于 WebGPU 的浏览器本地 AI 对话应用，使用 WebLLM 技术在客户端运行大语言模型。无需联网，完全在浏览器端完成 AI 推理，保护用户隐私。

### 1.2 技术架构

| 技术 | 版本/说明 |
|------|----------|
| UI 框架 | React 18 (CDN) |
| 样式框架 | Tailwind CSS (CDN) |
| AI 推理引擎 | WebLLM @mlc-ai/web-llm@0.2.79 |
| 语义向量引擎 | Transformers.js @xenova/transformers |
| 语义向量模型 | Xenova/paraphrase-multilingual-MiniLM-L12-v2 |
| JSX 编译 | Babel Standalone |
| 图标库 | Lucide Icons |
| 数据存储 | IndexedDB + localStorage |

### 1.3 项目结构

```
WebGPUAi/
├── webGPU.html          # 主应用（单文件）
├── PROJECT.md           # 项目文档
├── CLAUDE.md            # 开发规范
├── REQUIREMENTS.md      # 需求文档
├── RAG_IMPLEMENTATION.md # RAG 实现总结
├── Ming.jpg             # 头像图片
└── knowledgeBase/       # 知识库测试文件
    ├── test_knowledge_base.md
    ├── medical_knowledge.md
    ├── pudding_unique_test.md
    └── semantic_test.md   # 语义检索测试文件
```

---

## 二、功能清单

### 2.1 核心功能

| 功能 | 状态 | 说明 |
|------|------|------|
| 本地 AI 对话 | ✅ 已完成 | 基于 WebLLM 的浏览器端 LLM 推理 |
| 多模型支持 | ✅ 已完成 | 支持 Qwen2.5、Phi-3.5 等多种模型 |
| 流式响应 | ✅ 已完成 | 支持打字机效果的流式输出 |
| 模型缓存 | ✅ 已完成 | 使用 IndexedDB 缓存下载的模型 |
| 模型管理 | ✅ 已完成 | 查看缓存模型、删除模型、清理缓存 |

### 2.2 对话功能

| 功能 | 状态 | 说明 |
|------|------|------|
| 角色选择 | ✅ 已完成 | 内置多种 AI 角色（默认、程序员、作家等） |
| 系统提示词 | ✅ 已完成 | 可自定义系统提示词 |
| 对话历史 | ✅ 已完成 | IndexedDB 持久化存储 |
| 历史记录管理 | ✅ 已完成 | 查看、加载、删除历史对话 |
| 清空全部历史 | ✅ 已完成 | 一键清空所有历史对话 |
| 新建对话 | ✅ 已完成 | 清空当前对话开始新会话 |
| 消息复制 | ✅ 已完成 | 一键复制 AI 回复 |
| 思考模式 | ✅ 已完成 | 显示 AI 思考过程 |

### 2.3 UI/UX 功能

| 功能 | 状态 | 说明 |
|------|------|------|
| 暗黑/亮色主题 | ✅ 已完成 | 支持主题切换，带动画效果 |
| 响应式布局 | ✅ 已完成 | 适配不同屏幕尺寸 |
| 加载进度显示 | ✅ 已完成 | 显示模型下载/加载进度 |
| 首屏加载动画 | ✅ 已完成 | 防止主题闪烁的启动遮罩 |
| 模型就绪状态保持 | ✅ 已完成 | 刷新页面无需重新加载模型 |

### 2.4 知识库功能 (RAG)

| 功能 | 状态 | 说明 |
|------|------|------|
| 创建知识库 | ✅ 已完成 | 新建知识库集合 |
| 删除知识库 | ✅ 已完成 | 级联删除知识库及文档 |
| 知识库列表 | ✅ 已完成 | 显示所有知识库 |
| 文档上传 | ⚠️ 部分 | 支持 TXT、MD，**不支持 PDF** |
| 文档删除 | ✅ 已完成 | 删除知识库中的文档 |
| 文本分块 | ✅ 已完成 | 自动将文档分成 500 字符片段 |
| 关键词检索 | ✅ 已完成 | 基于关键词的文本检索（备用） |
| 语义检索 | ✅ 已完成 | 基于 Transformers.js 语义向量检索 |
| 知识库开关 | ✅ 已完成 | 滑块开关控制知识库启用 |
| 引用来源展示 | ✅ 已完成 | 显示 AI 回复的参考来源（折叠展开） |
| 检索增强 | ✅ 已完成 | 将检索内容注入 Prompt |
| 语义模型下载 | ✅ 已完成 | 手动下载语义向量模型 |
| 查询重写 | ✅ 已完成 | 基于对话历史的智能查询优化 |
| Web Worker 优化 | ✅ 已完成 | 检索计算移至后台线程，减少主线程卡顿 |

### 2.5 MCP 功能 (Model Context Protocol)

| 功能 | 状态 | 说明 |
|------|------|------|
| MCP 服务器配置 | ✅ 已完成 | 支持添加/删除多个 MCP 服务器 |
| 测试连接 | ✅ 已完成 | 添加服务器前验证连接 |
| 连接管理 | ✅ 已完成 | 单独连接/断开服务器 |
| 自动重连 | ✅ 已完成 | 刷新页面后自动连接已保存服务器 |
| 工具调用 | ✅ 已完成 | 自动检测关键词并调用 MCP 工具 |
| MCP 优先级 | ✅ 已完成 | MCP 优先于知识库，避免冲突 |
| 本地 MCP 服务器 | ✅ 已完成 | 提供搜索、天气、时间功能 |

**可用 MCP 工具：**
- `web_search` - 互联网搜索
- `get_weather` - 天气查询
- `get_time` - 获取时间

---

## 三、模型列表

### 3.1 LLM 模型 - 低显存 (< 2GB)

| 模型 ID | 名称 | 显存需求 | 特点 |
|--------|------|---------|------|
| Qwen2.5-0.5B | Qwen2.5 0.5B | ~1GB | 轻量快速 |
| Qwen2-1.5B | Qwen2 1.5B | ~2GB | 平衡选择 |
| Phi-3.5-mini | Phi-3.5 Mini | ~2GB | 微软模型 |

### 3.2 LLM 模型 - 高显存 (> 2GB)

| 模型 ID | 名称 | 显存需求 | 特点 |
|--------|------|---------|------|
| Qwen2-3B | Qwen2 3B | ~3GB | 中等性能 |
| Qwen3-4B | Qwen3 4B | ~4GB | 性能最强 |
| Phi-3-medium | Phi-3 Medium | ~6GB | 高性能 |

### 3.3 Embedding 模型

| 模型 ID | 名称 | 特点 |
|--------|------|------|
| paraphrase-multilingual-MiniLM-L12-v2 | 多语言语义向量模型 | 支持中文、量化压缩 (~100MB) |

---

## 四、角色列表

| 角色 ID | 名称 | 系统提示词 |
|--------|------|-----------|
| default | 默认 | 无 |
| programmer | 程序员 | 编程专家，帮助解决代码问题 |
| writer | 作家 | 文字创作，写文章、故事等 |
| teacher | 老师 | 教育专家，耐心解答问题 |
| lawyer | 律师 | 法律咨询，解答法律问题 |

---

## 五、技术实现细节

### 5.1 检索算法

**关键词与语义融合检索** - 基于 Transformers.js + Web Worker

1. **查询重写**：结合对话历史，将指代性问题重写为独立检索词
   - 智能判断：只有包含指代词（它/这个/那个）或省略表达时才重写
   - 保留核心意图和关键词

2. **语义检索优先**（核心检索方式）：
   - 模型：Xenova/bge-small-zh-v1.5（中文优化，向量模型自动处理同义词/语义相似）
   - 相似度阈值：35%（平衡召回与精确，避免无关内容干扰）
   - 余弦相似度计算
   - 权重：70%

3. **关键词检索补充**（仅做精确匹配）：
   - 2-4 字滑动窗口关键词提取
   - 评分：精确匹配 +30、关键词 +5/词、位置奖励
   - 仅当关键词精确匹配分数 >= 15 时优先使用
   - 权重：30%

4. **融合策略**：
   - 语义相似度 >= 20% → 直接使用语义结果
   - 关键词精确匹配分数 >= 15 → 优先使用关键词
   - 否则归一化后合并（语义 70% + 关键词 30%）

### 5.2 Web Worker 优化

- **内联 Worker**：使用 Blob URL 创建，无需额外文件
- **Worker 任务**：关键词检索、语义相似度计算、文本分块
- **主线程任务**：UI 渲染、WebGPU 推理、Embedding 计算
- **消息传递**：Promise 封装，支持 async/await 调用

### 5.3 Embedding 计算优化

- **并行批量计算**：每批 4 个文本同时计算
- **IndexedDB 存储**：向量数据持久化存储
- **按需计算**：文档上传时自动计算并存储

### 5.3 数据存储

| 数据类型 | 存储位置 | 键名 |
|---------|---------|------|
| 对话历史 | IndexedDB `pudding-chat-db` | `conversations` |
| 模型缓存 | IndexedDB `webllm-cache` | `model-cache` |
| 知识库 | IndexedDB `pudding-knowledge-db` | `knowledgeBases`, `documents`, `chunks` |
| 用户偏好 | localStorage | `pudding_settings`, `pudding_theme`, `pudding_knowledge_enabled` 等 |
| 语义模型状态 | localStorage | `pudding_embedding_model_loaded`, `pudding_embedding_model_version` |

### 5.4 主题初始化

- HTML 标签默认 `class="dark"`
- 内联 `<style>` 紧急样式防止闪烁
- 启动遮罩层在 React 渲染后淡出

---

## 六、后续开发计划

### 6.1 重要 (P1)

| 功能 | 优先级 | 说明 |
|------|--------|------|
| PDF 支持 | P1 | 集成 PDF.js 解析 PDF 文档 |
| 检索配置 | P1 | 可配置返回结果数量、相似度阈值 |

### 6.2 长期 (P2)

| 功能 | 优先级 | 说明 |
|------|--------|------|
| 文档预览 | P2 | 点击文档查看内容片段 |
| 多知识库 | P2 | 同时检索多个知识库 |
| 存储统计 | P2 | 显示知识库占用空间 |
| 分词优化 | P2 | 使用结巴分词提升中文分词质量 |

---

## 七、已知限制

### 7.1 技术限制

1. **纯前端方案** - 所有数据存储在浏览器 IndexedDB 中
2. **文件格式** - 暂只支持 TXT 和 MD，PDF 需要集成 PDF.js
3. **存储空间** - 受浏览器 IndexedDB 配额限制
4. **首次加载** - 语义模型约 100MB，需首次下载

### 7.2 浏览器要求

- Chrome 113+ 或 Edge 113+（需要 WebGPU 支持）
- 需要支持 WebGPU 的显卡

---

## 八、版本历史

| 版本 | 日期 | 更新内容 |
|------|------|---------|
| v1.0 | 2024-xx-xx | 初始版本，完成核心对话功能 |
| v1.1 | 2024-xx-xx | 添加知识库功能（RAG） |
| v1.2 | 2024-xx-xx | 添加主题切换、思考模式 |
| v1.3 | 2024-xx-xx | 添加模型状态保持、首屏优化 |
| v1.4 | 2025-xx-xx | 添加语义检索（Transformers.js）、清空全部历史 |
| v1.5 | 2025-02-25 | 添加查询重写、Web Worker 性能优化、语义检索优先策略 |
| v1.6 | 2026-02-26 | 添加 MCP 功能（多服务器支持、工具调用）、MCP 与知识库优先级处理、本地 MCP 服务器 |
| v1.6.1 | 2026-02-26 | 修复历史记录保存问题（添加防抖和并发保护）、优化 MCP 工具回复（天气、时间、搜索增加详细建议） |
| v1.6.2 | 2026-02-26 | 修复历史记录重复创建问题（同一会话只产生一条历史）、点击历史继续对话不产生新记录、实时保存用户消息和AI回复、排除空会话和系统提示的会话内容 |

---

*本项目为单文件 React 应用，所有代码位于 webGPU.html 中（mcp-server 为独立 Node.js 后端服务）*
